{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this seminar we will implement 2 most confusing but very popular layers - BatchNorm and Dropout along with the most popular architectures such as VGG, Inception, ResNet, ResNeXt, DenseNet, SENet and Mobilenet_v2.\n",
    "\n",
    "The main goal of this seminar is to make you understand how simple is to create your **arbitrary** neural network in pytorch and to give you some practical experience. PyTorch is very flexible and still simple, so you can implement any idea in just a few lines of code!\n",
    "\n",
    "Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 0., 2., 0., 2., 2., 0., 2., 0., 2.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # there is F.dropout(...) in torch, but let's not use it here\n",
    "        # use self.training flag\n",
    "        # TODO(students)\n",
    "        \n",
    "        if self.training:\n",
    "            mask = torch.empty(x.size()).bernoulli(1. - self.p)\n",
    "            return x * mask / (1. - self.p)\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "t = torch.ones(10)\n",
    "d = Dropout(p=0.5)\n",
    "d(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.eval()\n",
    "d(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "\n",
    "# TODO(me) still good realization somewhere\n",
    "class BatchNorm2D(nn.Module):\n",
    "    def __init__(self, num_features, momentum=0.1, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        self.gamma = Parameter(torch.Tensor(1, num_features, 1, 1), )\n",
    "        self.beta = Parameter(torch.Tensor(1, num_features, 1, 1))\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features, 1, 1))\n",
    "        self.register_buffer('running_var', torch.ones(num_features, 1, 1))\n",
    "        self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_running_stats(self):\n",
    "        self.running_mean.zero_()\n",
    "        self.running_var.fill_(1)\n",
    "        self.num_batches_tracked.zero_()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        init.ones_(self.gamma)\n",
    "        init.zeros_(self.beta)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # again, do not use F.batchnorm\n",
    "        # use self.training flag, don't forget to update running mean and variance while training\n",
    "        x_ch = x.transpose(0,1).contiguous().view(x.size(1), -1)\n",
    "        if self.training:\n",
    "            mean = x_ch.mean(1)[:, None, None]\n",
    "            var = x_ch.var(1)[:, None, None]\n",
    "            \n",
    "            self.running_mean = self.momentum * mean + (1. - self.momentum) * self.running_mean\n",
    "            self.running_var = self.momentum * var + (1. - self.momentum) * self.running_var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_std\n",
    "            \n",
    "        x = (x - mean) / (torch.sqrt(var + self.eps))\n",
    "        return self.gamma * x + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(40).float().reshape(2, 5, 2, 2)\n",
    "bn = BatchNorm2D(5)\n",
    "bn_torch = nn.BatchNorm2d(5)\n",
    "normed = bn(t)\n",
    "normed_torch = bn_torch(t)\n",
    "\n",
    "def approx_eq(a, b, eps=1e-5):\n",
    "    return (a.reshape(-1)-b.reshape(-1)).abs().sum() < eps\n",
    "assert approx_eq(bn.running_var, bn_torch.running_var)\n",
    "assert approx_eq(bn.running_mean, bn_torch.running_mean)\n",
    "assert torch.all(bn.gamma == bn_torch.weight)\n",
    "assert torch.all(bn.beta == bn_torch.bias)\n",
    "assert approx_eq(normed, normed_torch, eps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.1429, -1.0435],\n",
       "          [-0.9441, -0.8447]],\n",
       " \n",
       "         [[ 0.8447,  0.9441],\n",
       "          [ 1.0435,  1.1429]]], grad_fn=<SelectBackward>),\n",
       " tensor([[[-1.0691, -0.9761],\n",
       "          [-0.8831, -0.7902]],\n",
       " \n",
       "         [[ 0.7902,  0.8831],\n",
       "          [ 0.9761,  1.0691]]], grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_torch[:, 0], normed[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/alexnet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: torchvision\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "# AlexNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/vgg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, cfg, use_bn=False, num_classes=1000):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = VGG.make_layers(VGG.cfgs[cfg], batch_norm=use_bn)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    cfgs = {\n",
    "        'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "        'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_layers(cfg, batch_norm=False):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "# VGG('vgg16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have seen only sequential models which are essentially sequence of conv, pool, nonlinearity and normalization layers. Such architectures are in most cases very stable in training, but typically are slower, heavier and even converge to lower values. Modern architectures are somehow \"less sequential\", the main ideas are: compute several branches in parallel (inception), use information from previous layers and not only the last one (resnet, densenet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception\n",
    "\n",
    "![alt](images/we_need_to_go_deeper.jpeg \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Exceptional inception guide](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)\n",
    "\n",
    "Why only 3x3 convolutions? Maybe 5x5 will be better? Or 7x7? When you can not choose, it's time to take them all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](images/inception_naive.png \"\")\n",
    "![alt](images/inception_module.png \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: torchvision\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)\n",
    "    \n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj,\n",
    "                 conv_block=None):\n",
    "        super(Inception, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        # TODO(students): implement all branches based on the picture\n",
    "        self.branch1 = conv_block(in_channels, ch1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            conv_block(in_channels, ch3x3red, kernel_size=1),\n",
    "            conv_block(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            conv_block(in_channels, ch5x5red, kernel_size=1),\n",
    "            conv_block(ch5x5red, ch5x5, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),\n",
    "            conv_block(in_channels, pool_proj, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def _forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "\n",
    "        outputs = [branch1, branch2, branch3, branch4]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/googlenet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# googlelenet\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "    \"\"\"Intermediate 'heads' of googlenet\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, conv_block=None):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.conv = conv_block(in_channels, 128, kernel_size=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.adaptive_avg_pool2d(x, (4, 4))\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        x = F.dropout(x, 0.7, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        # N x 1000 (num_classes)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class GoogLeNet(nn.Module):\n",
    "    __constants__ = ['aux_logits', 'transform_input']\n",
    "\n",
    "    def __init__(self, num_classes=1000, aux_logits=True, transform_input=False, init_weights=True,\n",
    "                 blocks=None):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        if blocks is None:\n",
    "            blocks = [BasicConv2d, Inception, InceptionAux]\n",
    "        assert len(blocks) == 3\n",
    "        conv_block = blocks[0]\n",
    "        inception_block = blocks[1]\n",
    "        inception_aux_block = blocks[2]\n",
    "\n",
    "        self.aux_logits = aux_logits\n",
    "        self.transform_input = transform_input\n",
    "\n",
    "        self.conv1 = conv_block(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "        self.conv2 = conv_block(64, 64, kernel_size=1)\n",
    "        self.conv3 = conv_block(64, 192, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception3a = inception_block(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = inception_block(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception4a = inception_block(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = inception_block(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = inception_block(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = inception_block(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = inception_block(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.inception5a = inception_block(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = inception_block(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        if aux_logits:\n",
    "            self.aux1 = inception_aux_block(512, num_classes)\n",
    "            self.aux2 = inception_aux_block(528, num_classes)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # type: (Tensor) -> Tuple[Tensor, Tensor, Tensor]\n",
    "        # N x 3 x 224 x 224\n",
    "        x = self.conv1(x)\n",
    "        # N x 64 x 112 x 112\n",
    "        x = self.maxpool1(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.conv2(x)\n",
    "        # N x 64 x 56 x 56\n",
    "        x = self.conv3(x)\n",
    "        # N x 192 x 56 x 56\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        # N x 192 x 28 x 28\n",
    "        x = self.inception3a(x)\n",
    "        # N x 256 x 28 x 28\n",
    "        x = self.inception3b(x)\n",
    "        # N x 480 x 28 x 28\n",
    "        x = self.maxpool3(x)\n",
    "        # N x 480 x 14 x 14\n",
    "        x = self.inception4a(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        aux_defined = self.training and self.aux_logits\n",
    "        if aux_defined:\n",
    "            aux1 = self.aux1(x)\n",
    "        else:\n",
    "            aux1 = None\n",
    "\n",
    "        x = self.inception4b(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        x = self.inception4c(x)\n",
    "        # N x 512 x 14 x 14\n",
    "        x = self.inception4d(x)\n",
    "        # N x 528 x 14 x 14\n",
    "        if aux_defined:\n",
    "            aux2 = self.aux2(x)\n",
    "        else:\n",
    "            aux2 = None\n",
    "\n",
    "        x = self.inception4e(x)\n",
    "        # N x 832 x 14 x 14\n",
    "        x = self.maxpool4(x)\n",
    "        # N x 832 x 7 x 7\n",
    "        x = self.inception5a(x)\n",
    "        # N x 832 x 7 x 7\n",
    "        x = self.inception5b(x)\n",
    "        # N x 1024 x 7 x 7\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        # N x 1024 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 1024\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        # N x 1000 (num_classes)\n",
    "        return x, aux2, aux1\n",
    "    \n",
    "# GoogLeNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it look... complicated? It was the *first* inception network, but in the newer version (v2 and v3 appeared in the same paper) the authors suggested different versions of inception modules, by factorizing nxn convolutions in different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the even newer Inception_v4 paper authors introduced even more modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/inception_v4_blocks.jpeg)\n",
    "![](images/inception_v4_reduction_blocks.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/jackie_chan.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is still very simple to implement but may be a little bit confusing from the first glance. More importantly, why should this handcrafted design be optimal? This is one of the motivations for **Neural Architecture Search**. \n",
    "\n",
    "#### But anyway, let's look into other architecture ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blogposts to understand resnets:\n",
    "[1](https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624)\n",
    "[2](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea is to modify our layer transformation function in such a way that allows better and more stable training. Instead of learning transformation function $F(x)$ you learn another function $G(x)=F(x)-x$, but your layer is computed as $F(x)=G(x)+x$ which is the equivalent.\n",
    "\n",
    "There may be different orderings of the operations in the residual part. The one proposed in original paper was proved to converge worse with the increase of depth, to the extent that very deep ResNets were actually less powerfull than Resnet50. Among the other variants, pre-activation order (bn-relu-conv) converges faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/resnet_ordering.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(ch_in, ch_out, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(ch_in, ch_out, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "def get_skipconnection(ch_in, ch_out, stride):\n",
    "    if ch_in == ch_out and stride == 1:\n",
    "        return nn.Sequential()\n",
    "    return conv1x1(ch_in, ch_out, stride=stride)\n",
    "\n",
    "\n",
    "class ResNetBasicBlockDummy(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, stride=1):\n",
    "        super().__init__()\n",
    "        # Both self.conv1 and self.skipconnect layers downsample the input when stride != 1\n",
    "        # TODO(students): implement conv-bn-relu order (figure (c))\n",
    "        self.net = nn.Sequential(\n",
    "            conv3x3(ch_in, ch_out, stride=stride),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv3x3(ch_out, ch_out),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.skipconnect = get_skipconnection(ch_in, ch_out, stride)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x) + self.skipconnect(x)\n",
    "\n",
    "\n",
    "class ResNetBasicBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, stride=1):\n",
    "        super().__init__()\n",
    "        # Both self.conv1 and self.skipconnect layers downsample the input when stride != 1\n",
    "        # TODO(students): implement pre-activation order\n",
    "        self.net = nn.Sequential(\n",
    "            nn.BatchNorm2d(ch_in),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv3x3(ch_in, ch_out, stride=stride),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv3x3(ch_out, ch_out),\n",
    "        )\n",
    "        self.skipconnect = get_skipconnection(ch_in, ch_out, stride)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x) + self.skipconnect(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/resnet_architectures.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet18(\n",
       "  (net): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): ResNetBasicBlock(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (skipconnect): Sequential()\n",
       "    )\n",
       "    (2): ResNetBasicBlock(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (skipconnect): Sequential()\n",
       "    )\n",
       "    (3): ResNetBasicBlock(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (skipconnect): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (4): ResNetBasicBlock(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (skipconnect): Sequential()\n",
       "    )\n",
       "    (5): ResNetBasicBlock(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (skipconnect): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (6): ResNetBasicBlock(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (skipconnect): Sequential()\n",
       "    )\n",
       "    (7): ResNetBasicBlock(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (skipconnect): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (8): ResNetBasicBlock(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (skipconnect): Sequential()\n",
       "    )\n",
       "    (9): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (10): Flatten()\n",
       "    (11): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, n_classes=1000):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "            *self._make_layer(64, 64, 2),\n",
    "            *self._make_layer(64, 128, 2),\n",
    "            *self._make_layer(128, 256, 2),\n",
    "            *self._make_layer(256, 512, 2),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1000)\n",
    "        )\n",
    "        \n",
    "    def _make_layer(self, ch_in, ch_out, n_layers, stride=1):\n",
    "        layers = []\n",
    "        layers.append(ResNetBasicBlock(ch_in, ch_out, stride))\n",
    "        layers += [ResNetBasicBlock(ch_out, ch_out) for _ in range(n_layers - 1)]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "# ResNet18()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide Resnet\n",
    "Wider networks are more parallizable than deeper networks, just do wider layers. Also you may add dropout inside bottleneck layer to improve robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideResNetBasicBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, stride=1, p_drop=0.3):\n",
    "        super().__init__()\n",
    "        # Both self.conv1 and self.skipconnect layers downsample the input when stride != 1\n",
    "        # TODO(students): add dropout where appropriate\n",
    "        self.net = nn.Sequential(\n",
    "            nn.BatchNorm2d(ch_in),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv3x3(ch_in, ch_out, stride=stride),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv3x3(ch_out, ch_out),\n",
    "        )\n",
    "        self.skipconnect = get_skipconnection(ch_in, ch_out, stride)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x) + self.skipconnect(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNeXt\n",
    "One more idea is to apply group convolution (more parameter-efficient)\n",
    "![](images/resnext_block.png)\n",
    "![](images/resnext_block2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNeXtBottleneck(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, stride=1, groups=32, base_width=4):\n",
    "        super().__init__()\n",
    "        # Both self.conv1 and self.skipconnect layers downsample the input when stride != 1\n",
    "        # TODO(students): implement bottleneck (1x1->3x3->1x1) layer\n",
    "        ch_hid = int(ch_out * (base_width / 64.)) * groups\n",
    "        self.net = nn.Sequential(\n",
    "            nn.BatchNorm2d(ch_in),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv1x1(ch_in, ch_hid),\n",
    "            nn.BatchNorm2d(ch_hid),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv3x3(ch_hid, ch_hid, stride=stride, groups=groups),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.BatchNorm2d(ch_hid),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv1x1(ch_hid, ch_out),\n",
    "        )\n",
    "        self.skipconnect = get_skipconnection(ch_in, ch_out, stride)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x) + self.skipconnect(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Densenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Addition~~ -> Concatenation of all previous layers outputs\n",
    "\n",
    "In this architecture, the input of each layer consists of the feature maps of all earlier layer, and its output is passed to each subsequent layer. The feature maps are aggregated with depth-concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/densenet_layer.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Module):\n",
    "    \"\"\"1x1 -> 3x3 bottleneck\"\"\"\n",
    "    def __init__(self, ch_in, ch_out, ch_bn=4, s=1, p=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        ch_hid = ch_bn * ch_out  # channels after 1x1 convolution\n",
    " \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm2d(ch_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch_in, ch_hid, kernel_size=1),\n",
    "            nn.BatchNorm2d(ch_hid),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch_hid, ch_out, kernel_size=3, stride=s, padding=p)\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    " \n",
    " \n",
    "class TransitionLayer(nn.Module):\n",
    "    \"\"\"1x1 conv + average pool 2x2\"\"\"\n",
    "    def __init__(self, ch_in, ch_out, use_bn=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm2d(ch_in),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=1),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    " \n",
    " \n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, k_0, k=12, n=4):\n",
    "        super().__init__()\n",
    "        for l in range(n):\n",
    "            layer = DenseLayer(k_0 + k * l, k)\n",
    "            self.add_module('denselayer%d' % (l + 1), layer)\n",
    " \n",
    "    def forward(self, x):\n",
    "        for name, layer in self.named_children():\n",
    "            x = torch.cat((x, layer(x)), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/densenet_architectures.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (net): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): DenseBlock(\n",
       "      (denselayer1): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer2): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(76, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer3): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(88, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer4): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(100, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer5): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(112, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer6): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(124, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(124, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransitionLayer(\n",
       "      (conv): Sequential(\n",
       "        (0): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(136, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "    )\n",
       "    (3): DenseBlock(\n",
       "      (denselayer1): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer2): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(76, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer3): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(88, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer4): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(100, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer5): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(112, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer6): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(124, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(124, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer7): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(136, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer8): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(148, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(148, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer9): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(160, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer10): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(172, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(172, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer11): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(184, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer12): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(196, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(196, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransitionLayer(\n",
       "      (conv): Sequential(\n",
       "        (0): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(208, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "    )\n",
       "    (5): DenseBlock(\n",
       "      (denselayer1): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer2): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(76, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer3): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(88, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer4): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(100, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer5): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(112, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer6): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(124, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(124, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer7): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(136, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer8): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(148, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(148, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer9): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(160, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer10): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(172, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(172, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer11): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(184, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer12): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(196, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(196, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer13): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(208, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer14): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(220, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer15): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(232, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer16): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(244, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer17): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer18): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(268, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(268, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer19): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(280, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer20): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(292, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(292, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer21): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(304, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer22): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(316, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(316, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer23): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(328, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(328, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer24): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(340, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(340, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransitionLayer(\n",
       "      (conv): Sequential(\n",
       "        (0): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(352, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "    )\n",
       "    (7): DenseBlock(\n",
       "      (denselayer1): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer2): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(76, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer3): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(88, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer4): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(100, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer5): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(112, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer6): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(124, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(124, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer7): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(136, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer8): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(148, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(148, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer9): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(160, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer10): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(172, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(172, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer11): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(184, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer12): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(196, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(196, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer13): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(208, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer14): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(220, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(220, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer15): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(232, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (denselayer16): DenseLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): BatchNorm2d(244, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(244, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU()\n",
       "          (5): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (9): Flatten()\n",
       "    (10): Linear(in_features=256, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, ch_in=3, k=12, n_blocks=(6, 12, 24, 16), ch_mid=64, n_classes=1000):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(ch_in, ch_mid, kernel_size=(3, 3), padding=1),\n",
    "            DenseBlock(ch_mid, k=k, n=n_blocks[0])\n",
    "        ]\n",
    "        for i in range(len(n_blocks) - 1):\n",
    "            layers.extend([\n",
    "                TransitionLayer(ch_mid + k * n_blocks[i], ch_mid),\n",
    "                DenseBlock(ch_mid, k=k, n=n_blocks[i + 1])\n",
    "            ])\n",
    "            \n",
    "        layers += [\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(ch_mid + k * n_blocks[-1], 1000)\n",
    "        ]\n",
    "        self.net = nn.Sequential(*layers)\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "# DenseNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn which channels are important, implement Squeeze-and-Excitation module!\n",
    "![alt text](images/se_module.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now inject it into any architecture, e.g. into resnet bottleneck blocks and it will boost the performance by quite a margin!\n",
    "![alt text](images/se_inception_and_resnet.png \"Title\")\n",
    "![alt text](images/se_performance.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I can write models and modules and so what?\n",
    "How to compare your models and modules?\n",
    "- final network performance (your target metrics)\n",
    "- inference speed (in milliseconds on specific device)\n",
    "- FLOPS (rough approximation of real performance)\n",
    "- number of parameters (== weight of your model)\n",
    "- speed of training (how many epochs/time your model need to converge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy vs FLOPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/arch_chart.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def calc_trainable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence speed\n",
    "Add batch normalization layers to speed up model convergence (up to 10 times speed up in average). Essentially convolution and batch normalization both apply a linear transformation so you can \"fuse\" batch normalization into the neighbouring convolutional layer, modifying it's weights which will speed up your model inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical tips how to solve arbitrary DL problem\n",
    "1. start from a reasonable baseline (e.g. resnet50)\n",
    "2. improve target metrics as much as you can (your model may be big or slow, but you will now how good in theory your results should be)\n",
    "3. if necessary, compress your architecture and try smaller ones. Archieve reasonable speed and try to retain as much accuracy as you can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
