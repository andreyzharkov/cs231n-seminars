{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "colab_type": "code",
    "id": "1y1nzbJV0rtR",
    "outputId": "32e594d4-2863-4964-89b0-876b83b15271"
   },
   "outputs": [],
   "source": [
    "!pip install torchray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDf0-m_S0ZTl"
   },
   "outputs": [],
   "source": [
    "import torchray\n",
    "import torchray.benchmark\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N2sgj2SrbnrI"
   },
   "source": [
    "Let's take an example image with it's imagenet categories (`cat_id1` and `cat_id2` are just network output indices for some classes, contained in the image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "_NPSB1ea03fw",
    "outputId": "3410527c-9613-49c0-fc0a-b443f3fcfec8"
   },
   "outputs": [],
   "source": [
    "model, image_tensor, cat_id1, cat_id2 = torchray.benchmark.get_example_data(arch='vgg16', shape=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vQvdMcQR1KRc",
    "outputId": "6f1ef23e-422c-408b-938a-68c580c13157"
   },
   "outputs": [],
   "source": [
    "# image_tensor is already preprocessed appropriately with same normalization\n",
    "image_tensor.size(), cat_id1, cat_id2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WcrraWqqcDWl"
   },
   "source": [
    "## Vanilla gradient visualization\n",
    "Let's start from the simplest and most obvious visualization technique - the gradients themselves. We can backpropogate through our network until the input and use that input gradient as saliency map visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RAOXRP3lFOC5",
    "outputId": "cea3f13d-ac54-48c6-93be-f710aa79a9a4"
   },
   "outputs": [],
   "source": [
    "x = image_tensor\n",
    "category_id = cat_id1\n",
    "x.requires_grad_(True)\n",
    "# inference + backward\n",
    "y = model(x)\n",
    "z = y[0, category_id]\n",
    "z.backward()\n",
    "\n",
    "def to_saliency(gradient):\n",
    "    # gradient has shape (B, 3, H, W) but saliency should have shape (B, 1, H, W)\n",
    "    # and probably be non-negative\n",
    "    # how would you propose to compute saliency?\n",
    "    saliency = gradient.norm(dim=1, keepdim=True) \n",
    "    return saliency\n",
    "\n",
    "saliency = to_saliency(x.grad)\n",
    "saliency.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PHUv5LWc-bL"
   },
   "source": [
    "Let's plot what we have...\n",
    "\n",
    "The `plot_example` helper function simply plots two images and does nothing more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "QTYqCoBcF4Pr",
    "outputId": "4c286c1c-37da-41f7-bbbb-29cc2c8ad01c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "torchray.benchmark.plot_example(x, saliency, \n",
    "                                method=\"simple_backprop\", \n",
    "                                category_id=category_id, \n",
    "                                show_plot=True, \n",
    "                                save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "MyZgCfBDlD9x",
    "outputId": "d0e70113-be4f-4675-f843-64889651c04b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "torchray.benchmark.plot_example(x, x.grad, \n",
    "                                method=\"simple_backprop\", \n",
    "                                category_id=category_id, \n",
    "                                show_plot=True, \n",
    "                                save_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vTpS2DXIdQxf"
   },
   "source": [
    "You can try different ways to transform `grad` into `saliency`, check it out before continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "md2bkovXdgA1"
   },
   "source": [
    "## Changing backpropagation rules!\n",
    "\n",
    "As you can (or can not...) see from the above examples simple gradient visualization work somehow, but we believe the better way for visualization of saliency exists. One simple improvement we can do is to modify backpropagation rules slightly (for some specific layers/functions) to get more nice saliencies in the end.\n",
    "\n",
    "In `TorchRay` some methods are already implemented, let's check them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QcikZqbzehAj"
   },
   "outputs": [],
   "source": [
    "from torchray.attribution.common import gradient_to_saliency\n",
    "from torchray.benchmark import get_example_data, plot_example\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Sp4trEUexYx"
   },
   "source": [
    "The two well-known methods, namely Deconvnet and Guided Backprop modify only ReLU function backward computation.\n",
    "\n",
    "One way of changing backprop rules is to define custom context manager.\n",
    "\n",
    "So we are heading to something like this\n",
    "\n",
    "```python\n",
    "with ChangedBackpropRules():\n",
    "    y = model(x)\n",
    "    z = y[0, category_id]\n",
    "    z.backward()\n",
    "# and now y.grad contains *modified* gradient\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sGlPfYa5gsMX"
   },
   "source": [
    "How does this context manager look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TdLaLP7GehFL"
   },
   "outputs": [],
   "source": [
    "# `Patch` is a fancy tool to replace callable in a module\n",
    "class Patch(object):\n",
    "    \"\"\"Patch a callable in a module.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def resolve(target):\n",
    "        \"\"\"Resolve a target into a module and an attribute.\n",
    "        The function resolves a string such as ``'this.that.thing'`` into a\n",
    "        module instance `this.that` (importing the module) and an attribute\n",
    "        `thing`.\n",
    "        Args:\n",
    "            target (str): target string.\n",
    "        Returns:\n",
    "            tuple: module, attribute.\n",
    "        \"\"\"\n",
    "        target, attribute = target.rsplit('.', 1)\n",
    "        components = target.split('.')\n",
    "        import_path = components.pop(0)\n",
    "        target = __import__(import_path)\n",
    "        for comp in components:\n",
    "            import_path += '.{}'.format(comp)\n",
    "            __import__(import_path)\n",
    "            target = getattr(target, comp)\n",
    "        return target, attribute\n",
    "\n",
    "    def __init__(self, target, new_callable):\n",
    "        \"\"\"Patch a callable in a module.\n",
    "        Args:\n",
    "            target (str): path to the callable to patch.\n",
    "            callable (fun): new callable.\n",
    "        \"\"\"\n",
    "        target, attribute = Patch.resolve(target)\n",
    "        self.target = target\n",
    "        self.attribute = attribute\n",
    "        self.orig_callable = getattr(target, attribute)\n",
    "        setattr(target, attribute, new_callable)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.remove()\n",
    "\n",
    "    def remove(self):\n",
    "        \"\"\"Remove the patch.\"\"\"\n",
    "        if self.target is not None:\n",
    "            setattr(self.target, self.attribute, self.orig_callable)\n",
    "        self.target = None\n",
    "\n",
    "\n",
    "# This is our context manager (base class)\n",
    "# we will need to specify ReLU function implementation here\n",
    "class ReLUContext(object):\n",
    "    \"\"\"\n",
    "    A context manager that replaces :func:`torch.relu` with\n",
    "        :attr:`relu_function`.\n",
    "    Args:\n",
    "        relu_func (:class:`torch.autograd.function.FunctionMeta`): class\n",
    "            definition of a :class:`torch.autograd.Function`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, relu_func):\n",
    "        assert isinstance(relu_func, torch.autograd.function.FunctionMeta)\n",
    "        self.relu_func = relu_func\n",
    "        self.patches = []\n",
    "\n",
    "    def __enter__(self):\n",
    "        relu = self.relu_func().apply\n",
    "        self.patches = [\n",
    "            Patch('torch.relu', relu),\n",
    "            Patch('torch.relu_', relu),\n",
    "        ]\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        for p in self.patches:\n",
    "            p.remove()\n",
    "        return False  # re-raise any exception\n",
    "\n",
    "\n",
    "# Our fancy ReLU with changed backward pass\n",
    "class DeConvNetReLU(torch.autograd.Function):\n",
    "    \"\"\"DeConvNet ReLU autograd function.\n",
    "    This is an autograd function that redefines the ``relu`` function\n",
    "    to match the DeConvNet ReLU definition.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"DeConvNet ReLU forward function.\"\"\"\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"DeConvNet ReLU backward function.\"\"\"\n",
    "        return ?  # TODO\n",
    "\n",
    "\n",
    "# And finally our context manager which\n",
    "# swaps the `relu` implementation\n",
    "class DeConvNetContext(ReLUContext):\n",
    "    \"\"\"DeConvNet context.\n",
    "    This context modifies the computation of gradient to match the DeConvNet\n",
    "    definition.\n",
    "    See :mod:`torchray.attribution.deconvnet` for how to use it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DeConvNetContext, self).__init__(DeConvNetReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j3Q2zCcx4IbO"
   },
   "source": [
    "Note the difference:\n",
    "\n",
    "![ReLU backprop changed](https://cdn1.imggmi.com/uploads/2019/12/10/7ebba954e965228e071c407189f84986-full.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "DlsuukwoehDD",
    "outputId": "8e225800-5a6d-4529-fcbd-d6891a6d380a"
   },
   "outputs": [],
   "source": [
    "# Obtain example data.\n",
    "model, x, category_id, _ = get_example_data()\n",
    "\n",
    "# DeConvNet method.\n",
    "x.requires_grad_(True)\n",
    "\n",
    "with DeConvNetContext():\n",
    "    y = model(x)\n",
    "    z = y[0, category_id]\n",
    "    z.backward()\n",
    "\n",
    "saliency = gradient_to_saliency(x)\n",
    "\n",
    "# Plots.\n",
    "plt.figure(figsize=(15,15))\n",
    "plot_example(x, saliency, 'deconvnet', category_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZBGmxFojNkR"
   },
   "source": [
    "In `TorchRay` Deconvnet is already implemented, let's check ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "cGBIp_nNjBrD",
    "outputId": "bfd33c37-db60-4ca6-8596-e0e377781e1d"
   },
   "outputs": [],
   "source": [
    "from torchray.attribution.deconvnet import DeConvNetContext\n",
    "\n",
    "# Obtain example data.\n",
    "model, x, category_id, _ = get_example_data()\n",
    "\n",
    "# DeConvNet method.\n",
    "x.requires_grad_(True)\n",
    "\n",
    "with DeConvNetContext():\n",
    "    y = model(x)\n",
    "    z = y[0, category_id]\n",
    "    z.backward()\n",
    "\n",
    "saliency = gradient_to_saliency(x)\n",
    "\n",
    "# Plots.\n",
    "plt.figure(figsize=(15,15))\n",
    "plot_example(x, saliency, 'deconvnet', category_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPxDhLeDj8YC"
   },
   "source": [
    "Let's implement GuidedBackprop now, it is very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxHL8kgkehOC"
   },
   "outputs": [],
   "source": [
    "class GuidedBackpropReLU(torch.autograd.Function):\n",
    "    \"\"\"This class implements a ReLU function with the guided backprop rules.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"Guided backprop ReLU forward function.\"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"Guided backprop ReLU backward function.\"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        # TODO\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class GuidedBackpropContext(ReLUContext):\n",
    "    r\"\"\"GuidedBackprop context.\n",
    "    This context modifies the computation of gradients\n",
    "    to match the guided backpropagaton definition.\n",
    "    See :mod:`torchray.attribution.guided_backprop` for how to use it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GuidedBackpropContext, self).__init__(GuidedBackpropReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "EOsuYtCAkGRx",
    "outputId": "6d7afafe-b653-4234-ddfb-f33467c9fe7e"
   },
   "outputs": [],
   "source": [
    "model, image_tensor, cat_id1, cat_id2 = torchray.benchmark.get_example_data(arch='vgg16', shape=224)\n",
    "\n",
    "x = image_tensor\n",
    "category_id = cat_id1\n",
    "x.requires_grad_(True)\n",
    "\n",
    "with GuidedBackpropContext():\n",
    "      y = model(x)\n",
    "      z = y[0, category_id]\n",
    "      z.backward()\n",
    "# now compute saliency map from gradient\n",
    "saliency = gradient_to_saliency(x)\n",
    "saliency_cat1 = saliency\n",
    "# plot saliency for category1(dog)\n",
    "plt.figure(figsize=(15,15))\n",
    "torchray.benchmark.plot_example(x, saliency, \n",
    "                                method=\"guided_backprop\", \n",
    "                                category_id=category_id, \n",
    "                                show_plot=True, \n",
    "                                save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "3hB9cbUxkGUL",
    "outputId": "eebdb5f0-2bd1-4c14-c578-f553b44e7595"
   },
   "outputs": [],
   "source": [
    "# repeat for other category\n",
    "x = image_tensor\n",
    "category_id = cat_id2\n",
    "# note that we zero gradients here \n",
    "# as `x` already havs .grad from previous computations\n",
    "x.grad.zero_()\n",
    "model.zero_grad()\n",
    "\n",
    "with GuidedBackpropContext():\n",
    "      y = model(x)\n",
    "      z = y[0, category_id]\n",
    "      z.backward()\n",
    "# now compute saliency map from gradient\n",
    "saliency = gradient_to_saliency(x)\n",
    "plt.figure(figsize=(15,15))\n",
    "torchray.benchmark.plot_example(x, saliency, \n",
    "                                method=\"guided_backprop\", \n",
    "                                category_id=category_id, \n",
    "                                show_plot=True, \n",
    "                                save_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFOvXztq8lbb"
   },
   "source": [
    "Note how similar are 2 saliencies for different catagories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "9WTVMRf-1cGj",
    "outputId": "fd98a157-d988-443c-9682-e3bb60f5b476"
   },
   "outputs": [],
   "source": [
    "print(saliency.min(), saliency.max())\n",
    "print(saliency.size())\n",
    "saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "gAnJUg7c8ZRu",
    "outputId": "139b836a-94f7-4c78-f15a-1b456c4ecc4a"
   },
   "outputs": [],
   "source": [
    "saliency_cat1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tzC0lkTlUjW"
   },
   "source": [
    "Obviously `GuidedBackprop` is also already implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o8k9GYeehJ7"
   },
   "outputs": [],
   "source": [
    "from torchray.attribution.guided_backprop import GuidedBackpropContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zPHm5Ae2mGSZ"
   },
   "source": [
    "Before we move forward compare the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "pRhIeNncmLZX",
    "outputId": "62402c1c-61aa-44d6-e997-9ca93b0b729a"
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "# Obtain example data.\n",
    "model, x, category_id, _ = get_example_data()\n",
    "\n",
    "# Guided backprop.\n",
    "x.requires_grad_(True)\n",
    "\n",
    "with GuidedBackpropContext():\n",
    "    y = model(x)\n",
    "    z = y[0, category_id]\n",
    "    z.backward()\n",
    "\n",
    "saliency = gradient_to_saliency(x)\n",
    "\n",
    "# Plots.\n",
    "plot_example(x, saliency, 'guided backprop', category_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "3-kNH8JWmme7",
    "outputId": "c99e01f2-8ea1-4f00-fef1-8c13022527c5"
   },
   "outputs": [],
   "source": [
    "from torchray.attribution.guided_backprop import guided_backprop\n",
    "# 2\n",
    "# Obtain example data.\n",
    "model, x, category_id, _ = get_example_data()\n",
    "\n",
    "# Guided backprop.\n",
    "saliency = guided_backprop(model, x, category_id)\n",
    "\n",
    "# Plots.\n",
    "plot_example(x, saliency, 'guided backprop', category_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJtY8whDm8Fb"
   },
   "source": [
    "So just to write a bit less code let's import more general method which wraps up this forward + backward calls inside the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "qlpjUt2-myj1",
    "outputId": "644d4e68-b65e-4aad-c7f5-8aa1fa8cdae2"
   },
   "outputs": [],
   "source": [
    "from torchray.attribution.common import saliency as compute_saliency\n",
    "# Obtain example data.\n",
    "model, x, category_id, _ = get_example_data()\n",
    "\n",
    "# Guided backprop.\n",
    "saliency = compute_saliency(model, x, category_id, context_builder=GuidedBackpropContext)\n",
    "# saliency = guided_backprop(model, x, category_id)  # (the same)\n",
    "\n",
    "# Plots.\n",
    "plot_example(x, saliency, 'guided backprop', category_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1V3rsT3I8ur8"
   },
   "source": [
    "Let's use and compare other models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OkEdhrVvnyNa"
   },
   "outputs": [],
   "source": [
    "# gradient\n",
    "from torchray.attribution.gradient import gradient\n",
    "# gradient with different grad2saliency function\n",
    "from torchray.attribution.grad_cam import grad_cam\n",
    "from torchray.attribution.linear_approx import linear_approx\n",
    "# ReLU overwritten\n",
    "from torchray.attribution.deconvnet import deconvnet\n",
    "from torchray.attribution.guided_backprop import guided_backprop\n",
    "# Linear layers overwritten\n",
    "from torchray.attribution.excitation_backprop import excitation_backprop\n",
    "from torchray.attribution.excitation_backprop import contrastive_excitation_backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h9V3EpMIkbJ5"
   },
   "source": [
    "You can see `TorchRay` [docs](https://facebookresearch.github.io/TorchRay/attribution.html#) for method details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KzT6CyxhnyQC"
   },
   "outputs": [],
   "source": [
    "def draw_method(model, x, category_id, method, method_name, figsize=(15,15), **kwargs):\n",
    "    saliency = method(model, x, category_id, **kwargs)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plot_example(x, saliency, method_name, category_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "mj7OekjPnutu",
    "outputId": "56ee063b-d0d6-45fb-85fe-302ef759fe80"
   },
   "outputs": [],
   "source": [
    "model, x, category_id, _ = get_example_data()\n",
    "draw_method(model, x, category_id, method=gradient, method_name=\"gradient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SE7M9cNEnySh",
    "outputId": "903ed903-36f6-4ee0-ec7b-cb96e02b2231"
   },
   "outputs": [],
   "source": [
    "methods = [gradient, linear_approx, deconvnet, excitation_backprop]\n",
    "method_names = [\"gradient\", \"linear_approx\", \"deconvnet\", \"excitation_backprop\"]\n",
    "\n",
    "archs = ['alexnet', 'vgg11', 'resnet18', 'resnet50', 'wide_resnet101_2', 'densenet121', 'densenet201', 'mobilenet_v2']\n",
    "\n",
    "for arch in archs:\n",
    "    model, image_tensor, cat_id1, cat_id2 = torchray.benchmark.get_example_data(arch=arch, shape=224)\n",
    "    print(f\"Model={arch}\", flush=True)\n",
    "    for method, method_name in zip(methods, method_names):\n",
    "        draw_method(model, image_tensor, cat_id1, method=method, method_name=method_name)\n",
    "        if image_tensor.grad is not None:\n",
    "            image_tensor.grad.zero_()\n",
    "        draw_method(model, image_tensor, cat_id2, method=method, method_name=method_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U89v--Ns-6vx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mW0LvE6rtFcb"
   },
   "source": [
    "For now we visualize only the saliency on the first layer, but can we do this for some intermediate layer? \n",
    "\n",
    "Yes, we do! Let's check it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "JyiLKAPhCx-u",
    "outputId": "b33457f4-0651-4c77-b0eb-540359a97997"
   },
   "outputs": [],
   "source": [
    "model, image_tensor, cat_id1, cat_id2 = torchray.benchmark.get_example_data()\n",
    "# for method, method_name in zip(methods, method_names):\n",
    "draw_method(model, image_tensor, cat_id1, method=guided_backprop, method_name=\"gradient\", saliency_layer=\"features.9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "colab_type": "code",
    "id": "fOGNXbwJuM_F",
    "outputId": "c61eed4b-f2d0-4b5d-d57e-658f0977411b"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5bplXa3uRja"
   },
   "source": [
    "Now try to visualize on all non-activation layers of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Hs_LNaecuPAp",
    "outputId": "7cdc3ea1-1173-4c76-ab93-11cb1a171c55"
   },
   "outputs": [],
   "source": [
    "model, image_tensor, cat_id1, cat_id2 = torchray.benchmark.get_example_data()\n",
    "method = guided_backprop\n",
    "method_name = \"gradient\"\n",
    "# iterate throuwh all non-relu layers\n",
    "for i in range(31):\n",
    "    if image_tensor.grad is not None:\n",
    "        image_tensor.grad.zero_()\n",
    "\n",
    "    layer_is_not_relu = ???  # TODO\n",
    "    if layer_is_not_relu:\n",
    "        saliency_layer = f\"features.{i}\"\n",
    "        print(f\"Saliency layer: {saliency_layer}\", flush=True)\n",
    "        draw_method(model, image_tensor, cat_id1, method=method, method_name=method_name, saliency_layer=saliency_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wuoG2NuwyY3"
   },
   "source": [
    "You can also try with other methods and architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "sYO5wxWWwV8k",
    "outputId": "7cc3783c-6a03-4352-ef08-336360eb533e"
   },
   "outputs": [],
   "source": [
    "model, image_tensor, cat_id1, cat_id2 = torchray.benchmark.get_example_data()\n",
    "draw_method(model, image_tensor, cat_id1, method=excitation_backprop, method_name=\"gradient\", saliency_layer=\"features.30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sANX16kzC2YC"
   },
   "source": [
    "## Perturbation methods\n",
    "Perturbation methods **do not require your model to be differentiable, so you can actually apply it to any classifier**, aka black box model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "colab_type": "code",
    "id": "Qsnl2qN6x9SA",
    "outputId": "baadd496-9471-4368-cd79-5a4633267df6"
   },
   "outputs": [],
   "source": [
    "# RISE\n",
    "from torchray.attribution.rise import rise, rise_class\n",
    "from torchray.benchmark import get_example_data, plot_example\n",
    "from torchray.utils import get_device\n",
    "\n",
    "# Obtain example data.\n",
    "model, x, category_id_1, category_id_2 = get_example_data()\n",
    "\n",
    "# Run on GPU if available.\n",
    "device = get_device()\n",
    "model.to(device)\n",
    "x = x.to(device)\n",
    "\n",
    "# RISE method.\n",
    "saliency = rise(model, x)\n",
    "saliency1 = saliency[:, category_id_1].unsqueeze(0)\n",
    "saliency2 = saliency[:, category_id_2].unsqueeze(0)\n",
    "\n",
    "# Plots.\n",
    "plt.figure(figsize=(15,15))\n",
    "plot_example(x, saliency1, 'RISE', category_id_1)\n",
    "plt.figure(figsize=(15,15))\n",
    "plot_example(x, saliency2, 'RISE', category_id_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 870
    },
    "colab_type": "code",
    "id": "LamrVQ7tx9ZQ",
    "outputId": "481f1f1f-813b-4f59-d09d-2093b96172f1"
   },
   "outputs": [],
   "source": [
    "# RISE per class\n",
    "# Obtain example data.\n",
    "model, x, category_id_1, category_id_2 = get_example_data()\n",
    "\n",
    "# Run on GPU if available.\n",
    "device = get_device()\n",
    "model.to(device)\n",
    "x = x.to(device)\n",
    "\n",
    "# RISE method.\n",
    "saliency1 = rise_class(model, x, target=[category_id_1])  # should work with target=category_id_1, looks like a bug\n",
    "print(saliency1.size())\n",
    "saliency2 = rise_class(model, x, target=[category_id_2])\n",
    "\n",
    "# Plots.\n",
    "plt.figure(figsize=(15,15))\n",
    "plot_example(x, saliency1, 'RISE', category_id_1)\n",
    "plt.figure(figsize=(15,15))\n",
    "plot_example(x, saliency2, 'RISE', category_id_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kZaf9sca0Yuy"
   },
   "source": [
    "The torchray's authors are benchmarking several approaches and proposes the new method called \"Extremal Perturbation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "C3VzCWzlRWc2",
    "outputId": "57b81ccd-9790-4c17-d7de-3f6b07b98c96"
   },
   "outputs": [],
   "source": [
    "from torchray.attribution.extremal_perturbation import extremal_perturbation, contrastive_reward\n",
    "from torchray.benchmark import get_example_data, plot_example\n",
    "from torchray.utils import get_device\n",
    "\n",
    "# Obtain example data.\n",
    "model, x, category_id_1, category_id_2 = get_example_data()\n",
    "\n",
    "# Run on GPU if available.\n",
    "device = get_device()\n",
    "model.to(device)\n",
    "x = x.to(device)\n",
    "\n",
    "# Extremal perturbation backprop.\n",
    "masks_1, _ = extremal_perturbation(\n",
    "    model, x, category_id_1,\n",
    "    reward_func=contrastive_reward,\n",
    "    debug=True,\n",
    "    areas=[0.12],\n",
    ")\n",
    "\n",
    "masks_2, _ = extremal_perturbation(\n",
    "    model, x, category_id_2,\n",
    "    reward_func=contrastive_reward,\n",
    "    debug=True,\n",
    "    areas=[0.05],\n",
    ")\n",
    "\n",
    "# Plots.\n",
    "plot_example(x, masks_1, 'extremal perturbation', category_id_1)\n",
    "plot_example(x, masks_2, 'extremal perturbation', category_id_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JLmCvUNe5dH_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svIp9LAx5dQ-"
   },
   "source": [
    "So which method is better? TorchRay provide [benchmarking results](https://facebookresearch.github.io/TorchRay/benchmark.html#id8) in a \"pointing game\". In this game the goal of the method is to provide a saliency map on which pixel with maximal value will belong to an object of the specified class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t40hVVK6QGz"
   },
   "source": [
    "Beyond classification interpretability: can we interpret segmentation/detection models? Yes we can, but it is a harder task and the simple methods provide much worse results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7wskOMw08gl"
   },
   "source": [
    "# References & Further read\n",
    "\n",
    "1. TorchRay [github](https://github.com/facebookresearch/TorchRay/tree/master), [docs](https://facebookresearch.github.io/TorchRay/index.html)\n",
    "\n",
    "2. ICCV'19 interpretability tutorial (theory) [slides](https://interpretablevision.github.io/slide/iccv19_binder_slide.pdf) [site](https://interpretablevision.github.io/)\n",
    "\n",
    "3. Explainable AI: Interpreting, Explaining and Visualizing Deep Learning [book](https://www.springer.com/gp/book/9783030289539)\n",
    "\n",
    "4. Papers: [deconvnet](https://doi.org/10.1007/978-3-319-10590-1_53), [guided backprop](https://arxiv.org/abs/1412.6806), [grad-cam](http://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html), [excitation backprop](https://arxiv.org/abs/1608.00507), [RISE](https://arxiv.org/pdf/1806.07421.pdf), [Extremal Perturbations](https://arxiv.org/abs/1910.08485)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "schQB5-f3Hlb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TorchRay_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
