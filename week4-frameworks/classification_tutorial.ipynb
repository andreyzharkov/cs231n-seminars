{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MRuF-zzv0Ecf"
   },
   "source": [
    "# Catalyst classification tutorial\n",
    "\n",
    "This notebook is a modified version of [classification tutorial](https://github.com/catalyst-team/catalyst/blob/master/examples/notebooks/classification-tutorial.ipynb) authored by [Roman Tezikov](https://github.com/TezRomacH). Many thanks for the catalyst-team for their hard work!\n",
    "\n",
    "[![Catalyst logo](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst_logo.png)](https://github.com/catalyst-team/catalyst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3I6p805u0Ech"
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Download and install the latest version of catalyst and other libraries required for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IaP5eBt90Ech",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U catalyst\n",
    "!pip install albumentations\n",
    "!pip install pretrainedmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08DtiOuR0Eck"
   },
   "source": [
    "### Colab extras\n",
    "\n",
    "First of all, do not forget to change the runtime type to GPU. <br/>\n",
    "To do so click `Runtime` -> `Change runtime type` -> Select `\"Python 3\"` and `\"GPU\"` -> click `Save`. <br/>\n",
    "After that you can click `Runtime` -> `Run` all and watch the tutorial.\n",
    "\n",
    "\n",
    "To intergate visualization library `plotly` to colab, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BRDvILfV0Eck"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "def configure_plotly_browser_state():\n",
    "    display(IPython.core.display.HTML('''\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "        <script>\n",
    "          requirejs.config({\n",
    "            paths: {\n",
    "              base: '/static/base',\n",
    "              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',\n",
    "            },\n",
    "          });\n",
    "        </script>\n",
    "        '''))\n",
    "\n",
    "\n",
    "IPython.get_ipython().events.register('pre_run_cell', configure_plotly_browser_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MwR8EH4i0Ecm"
   },
   "source": [
    "## Setting up GPUs\n",
    "PyTorch and Catalyst versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2MqiphIu0Ecm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.2.0', '19.10.2')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, catalyst\n",
    "\n",
    "torch.__version__, catalyst.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PhfTyUsm0Eco"
   },
   "source": [
    "You can also specify GPU/CPU usage for this turorial.\n",
    "\n",
    "Available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t6Z-hwmf0Ecp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyarrow not available, switching to pickle. To install pyarrow, run `pip install pyarrow`.\n",
      "lz4 not available, disabling compression. To install lz4, run `pip install lz4`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catalyst.utils import get_available_gpus\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jGlYwYSe0Ecq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple, Callable\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # \"\" - CPU, \"0\" - 1 GPU, \"0,1\" - MultiGPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3r6l5JlT0Ecs"
   },
   "source": [
    "## Reproducibility first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R9UZs-I-0Ect"
   },
   "source": [
    "Catalyst provides a special utils for research results reproducibility. <br/>\n",
    "For example, `catalyst.utils.set_global_seed` fixes seed for all main DL frameworks (` PyTorch`, `Tensorflow`,` random` and `numpy`)\n",
    "\n",
    "In case of CuDNN you can set deterministic mode and flag to use benchmark with\n",
    "`catalyst.utils.prepare_cudnn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WjLuNZhy0Ect"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "from catalyst.utils import set_global_seed, prepare_cudnn\n",
    "\n",
    "set_global_seed(SEED)\n",
    "prepare_cudnn(deterministic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWLB45Md0Ecv"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6SyxPFjn0Ecv"
   },
   "source": [
    "In this tutorial we will use one of four datasets:\n",
    "\n",
    "- [Ants / Bees dataset](https://www.dropbox.com/s/ffzfpbwzwdo9qp8/ants_bees_cleared_190806.tar.gz )\n",
    "- [Stanford Dogs dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) \n",
    "- [Flowers Recognition](https://www.kaggle.com/alxmamaev/flowers-recognition)\n",
    "- [Best Artworks of All Time](https://www.kaggle.com/ikarus777/best-artworks-of-all-time)\n",
    "\n",
    "If you are on MacOS and you don’t have `wget`, you can install it with:` brew install wget`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VO7_rJ5r0Ecw"
   },
   "source": [
    "Let's specify the dataset by `DATASET` variable:\n",
    "- `ants_bees` (~ 400 pictures for 2 classes)  – for CPU experiments\n",
    "- `flowers` (~ 4k pictures for 5 classes)     – for fast GPU experiments \n",
    "- `dogs` (~ 20k pictures for 120 classes)\n",
    "- `artworks` (~ 8k pictures for 50 classes)   – for GPU experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnLqsRTh0Ecw"
   },
   "outputs": [],
   "source": [
    "DATASET = \"flowers\" # \"ants_bees\" / \"flowers\" / \"dogs\" / \"artworks\"\n",
    "os.environ[\"DATASET\"] = DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6lzJ6PnH0Ecy"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "function gdrive_download () {\n",
    "  CONFIRM=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \"https://docs.google.com/uc?export=download&id=$1\" -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')\n",
    "  wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$CONFIRM&id=$1\" -O $2\n",
    "  rm -rf /tmp/cookies.txt\n",
    "}\n",
    "\n",
    "mkdir -p Images\n",
    "rm -rf Images/\n",
    "\n",
    "if [[ \"$DATASET\" == \"ants_bees\" ]]; then\n",
    "    gdrive_download 1czneYKcE2sT8dAMHz3FL12hOU7m1ZkE7 ants_bees_cleared_190806.tar.gz\n",
    "    tar -xf ants_bees_cleared_190806.tar.gz &>/dev/null\n",
    "    mv ants_bees_cleared_190806 Images\n",
    "elif [[ \"$DATASET\" == \"flowers\" ]]; then\n",
    "    gdrive_download 1rvZGAkdLlbR_MEd4aDvXW11KnLaVRGFM flowers.tar.gz\n",
    "    tar -xf flowers.tar.gz &>/dev/null\n",
    "    mv flowers Images\n",
    "elif [[ \"$DATASET\" == \"dogs\" ]]; then\n",
    "    # https://www.kaggle.com/jessicali9530/stanford-dogs-dataset\n",
    "    wget http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n",
    "    tar -xf images.tar &>/dev/null\n",
    "elif [[ \"$DATASET\" == \"artworks\" ]]; then\n",
    "    # https://www.kaggle.com/ikarus777/best-artworks-of-all-time\n",
    "    gdrive_download 1eAk36MEMjKPKL5j9VWLvNTVKk4ube9Ml artworks.tar.gz\n",
    "    tar -xf artworks.tar.gz &>/dev/null\n",
    "    mv artworks Images\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vwfTtal0Ec0"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = \"Images/\"\n",
    "ALL_IMAGES = list(Path(ROOT).glob(\"**/*.jpg\"))\n",
    "ALL_IMAGES = list(filter(lambda x: not x.name.startswith(\".\"), ALL_IMAGES))\n",
    "print(\"Number of images:\", len(ALL_IMAGES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I4OfXcuH0Ec1"
   },
   "source": [
    "Let's check out the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YdHiXkHw0Ec2"
   },
   "outputs": [],
   "source": [
    "from catalyst.utils import imread\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_examples(images: List[Tuple[str, np.ndarray]]):\n",
    "    _indexes = [(i, j) for i in range(2) for j in range(2)]\n",
    "    \n",
    "    f, ax = plt.subplots(2, 2, figsize=(16, 16))\n",
    "    for (i, j), (title, img) in zip(_indexes, images):\n",
    "        ax[i, j].imshow(img)\n",
    "        ax[i, j].set_title(title)\n",
    "    f.tight_layout()\n",
    "\n",
    "def read_random_images(paths: List[Path]) -> List[Tuple[str, np.ndarray]]:\n",
    "    data = np.random.choice(paths, size=4)\n",
    "    result = []\n",
    "    for d in data:\n",
    "        title = f\"{d.parent.name}: {d.name}\"\n",
    "        _image = imread(d)\n",
    "        result.append((title, _image))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDbb4YAe0Ec4"
   },
   "source": [
    "You can restart the cell below to see more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ZN1tPRR0Ec5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images = read_random_images(ALL_IMAGES)\n",
    "show_examples(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DwlAHkw30Ec-"
   },
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DyRFACmY0EdQ"
   },
   "source": [
    "For augmentation of our dataset, we will use the [albumentations library](https://github.com/albu/albumentations).  <br/>\n",
    "You can view the list of available augmentations on the documentation [website](https://albumentations.readthedocs.io/en/latest/api/augmentations.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_kLIMUvg0EdR"
   },
   "outputs": [],
   "source": [
    "from albumentations import Compose, LongestMaxSize, PadIfNeeded\n",
    "from albumentations import ShiftScaleRotate, IAAPerspective, RandomBrightnessContrast, RandomGamma, \\\n",
    "    HueSaturationValue, ToGray, CLAHE, JpegCompression\n",
    "\n",
    "from albumentations import Normalize\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "BORDER_CONSTANT = 0\n",
    "BORDER_REFLECT = 2\n",
    "\n",
    "def pre_transforms(image_size=224):\n",
    "    # Convert the image to a square of size image_size x image_size\n",
    "    # (keeping aspect ratio)\n",
    "    result = [\n",
    "        LongestMaxSize(max_size=image_size),\n",
    "        PadIfNeeded(image_size, image_size, border_mode=BORDER_REFLECT)\n",
    "    ]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def hard_transforms():\n",
    "    result = [\n",
    "        # Random shifts, stretches and turns with a 50% probability\n",
    "        ShiftScaleRotate(\n",
    "            shift_limit=0.1,\n",
    "            scale_limit=0.1,\n",
    "            rotate_limit=15,\n",
    "            border_mode=BORDER_REFLECT,\n",
    "            p=0.5\n",
    "        ),\n",
    "        IAAPerspective(scale=(0.02, 0.05), p=0.3),\n",
    "        # Random brightness / contrast with a 30% probability\n",
    "        RandomBrightnessContrast(\n",
    "            brightness_limit=0.2, contrast_limit=0.2, p=0.3\n",
    "        ),\n",
    "        # Random gamma changes with a 30% probability\n",
    "        RandomGamma(gamma_limit=(85, 115), p=0.3),\n",
    "        # Randomly changes the hue, saturation, and color value of the input image\n",
    "        HueSaturationValue(p=0.3),\n",
    "        JpegCompression(quality_lower=80),\n",
    "    ]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def post_transforms():\n",
    "    # we use ImageNet image normalization\n",
    "    # and convert it to torch.Tensor\n",
    "    return [Normalize(), ToTensor()]\n",
    "\n",
    "def compose(_transforms):\n",
    "    # combine all augmentations into one single pipeline\n",
    "    result = Compose([item for sublist in _transforms for item in sublist])\n",
    "    return result\n",
    "\n",
    "\n",
    "train_transforms = compose([pre_transforms(), hard_transforms(), post_transforms()])\n",
    "valid_transforms = compose([pre_transforms(), post_transforms()])\n",
    "\n",
    "show_transforms = compose([pre_transforms(), hard_transforms()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8o-lBLMu0EdV"
   },
   "source": [
    "Let's look at the augmented results. <br/>\n",
    "The cell below can be restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "olOhKey40EdV"
   },
   "outputs": [],
   "source": [
    "images = read_random_images(ALL_IMAGES)\n",
    "\n",
    "images = [\n",
    "    (title, show_transforms(image=i)[\"image\"])\n",
    "    for (title, i) in images\n",
    "]\n",
    "show_examples(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RIcLTTN40Ec7"
   },
   "source": [
    "## Pytorch Dataset and DataLoaders\n",
    "Torchvision has standard ImageFolder Dataset class for the following folder structure:\n",
    "```\n",
    "dataset/\n",
    "    class_1/\n",
    "        *.ext\n",
    "        ...\n",
    "    class_2/\n",
    "        *.ext\n",
    "        ...\n",
    "    ...\n",
    "    class_N/\n",
    "        *.ext\n",
    "        ...\n",
    "```\n",
    "\n",
    "Let's use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JglcCCtYNCf_"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import cv2\n",
    "\n",
    "def read_image(path):\n",
    "    image_bgr = cv2.imread(path)\n",
    "    assert image_bgr is not None, f\"None image {path}\"\n",
    "    assert image_bgr.ndim == 3\n",
    "    assert image_bgr.dtype == np.uint8, \"Albumentations will have incorrect normalization\"\n",
    "    return cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "class AlbDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None,\n",
    "                 loader=None, is_valid_file=None):\n",
    "        # we want to override only the default loader here to read images as np.uint8 (by default torchvision reads PIL)\n",
    "        if loader is None:\n",
    "            loader = read_image\n",
    "        super().__init__(root, transform=transform, target_transform=target_transform, loader=loader, is_valid_file=is_valid_file)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        image = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            # if you use albumentations you will have to change your dataset form torchvision's defaults\n",
    "            # as self.transform is applied in different maneer\n",
    "            # or modify transformation function\n",
    "            # self.transform(**dict_input) -> dict_output\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        if self.target_transform is not None:\n",
    "            # maybe we will need target transform (like one-hot encoding or label smoothing)\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rGx7S1TW0Ec8"
   },
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "\n",
    "def has_image_ext(filename):\n",
    "    return os.path.splitext(filename)[1].lower() in (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "\n",
    "def is_test_file(filename):\n",
    "    # more smart and balanced way will be to split images in each folder separately\n",
    "    # but let's omit this for simplicity\n",
    "    # Warning: in your HW use the best splitting strategy you know, the one implemented here is not really a good one\n",
    "    if not has_image_ext(filename):\n",
    "        return False\n",
    "    return hash(filename) % int(1. / TEST_SIZE) == 0\n",
    "\n",
    "def is_train_file(filename):\n",
    "    return has_image_ext(filename) and not is_test_file(filename)\n",
    "\n",
    "test_dataset = AlbDataset(\"Images\", loader=read_image, transform=train_transforms, target_transform=None, is_valid_file=is_test_file)\n",
    "train_dataset = AlbDataset(\"Images\", loader=read_image, transform=valid_transforms, target_transform=None, is_valid_file=is_train_file)\n",
    "class_names = train_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "len(train_dataset), len(test_dataset), class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZR3eG2XWKW7J"
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DNl3pfMaEmLS"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch.utils.data._utils import default_collate\n",
    "\n",
    "# we want to obtain both `targets` and `targets_one_hot`\n",
    "# the more transparent (and correct) way to do so would be to modify the dataset's __getitem__\n",
    "def collate_fn(batch):\n",
    "    # batch == [dataset[idx1], dataset[idx2], ..., dataset[idxN]] where N is batch size\n",
    "    # in our case dataset[idxK] = (image [torch.tensor], label [int])\n",
    "\n",
    "    features, targets = list(zip(*batch))\n",
    "\n",
    "    features = torch.stack(features, dim=0)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "    targets_onehot = torch.zeros(targets.size(0), num_classes, dtype=torch.long, device=targets.device)\n",
    "    targets_onehot[torch.arange(targets.size(0), device=targets.device), targets] = 1\n",
    "    return {'features': features, 'targets': targets, 'targets_one_hot': targets_onehot}\n",
    "\n",
    "\n",
    "def get_loaders(batch_size=64, num_workers=4, sampler=None, **kwargs):\n",
    "    # we have to use ordered dict as the iteration goes `for loader in loaders`\n",
    "    # and the order is important (hope it will be fixed in the future)\n",
    "    loaders = OrderedDict()\n",
    "    loaders[\"train\"] = DataLoader(train_dataset, batch_size=batch_size, shuffle=sampler is None, sampler=sampler, num_workers=num_workers, **kwargs)\n",
    "    loaders[\"valid\"] = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, sampler=None, num_workers=num_workers, **kwargs)\n",
    "    return loaders\n",
    "\n",
    "loaders = get_loaders(collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SNtguZvp0Eda"
   },
   "source": [
    "## Model\n",
    "\n",
    "Let's take the classification model from [Cadene pretrain models](https://github.com/Cadene/pretrained-models.pytorch). This repository contains a huge number of pre-trained PyTorch models. <br/>\n",
    "But at first, let's check them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEvLgMle0Eda"
   },
   "outputs": [],
   "source": [
    "import pretrainedmodels\n",
    "\n",
    "pretrainedmodels.model_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47MiwEp60Edb"
   },
   "source": [
    "For this tutorial purposes, `ResNet18` is good enough, but you can try other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BC0bKKUq0Edc"
   },
   "outputs": [],
   "source": [
    "model_name = \"resnet18\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kCfIfcLP0Edd"
   },
   "source": [
    "By `pretrained_settings` we can see what the given network expects as input and what would be the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "18h1o1zR0Edd"
   },
   "outputs": [],
   "source": [
    "pretrainedmodels.pretrained_settings[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OtildH2c0Ede"
   },
   "source": [
    "The model returns logits for classification into 1000 classes from ImageNet. <br/>\n",
    "Let's define a function that will replace the last fully-conected layer for our number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Y1w0ckv0Edf"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "def get_model(model_name: str, num_classes: int, pretrained: str = \"imagenet\"):\n",
    "    model_fn = pretrainedmodels.__dict__[model_name]\n",
    "    model = model_fn(num_classes=1000, pretrained=pretrained)\n",
    "    \n",
    "    dim_feats = model.last_linear.in_features\n",
    "    model.last_linear = nn.Linear(dim_feats, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kq_3IYdm0Edg"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sfUoi8c90Edg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# model creation\n",
    "model = get_model(model_name, num_classes)\n",
    "\n",
    "# as we are working on basic classification problem (no multi-class/multi-label)\n",
    "# let's use standard CE loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer, milestones=[9], gamma=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h-polMKI0Edi"
   },
   "source": [
    "To run some DL experiment, Catalyst uses a [Runner](https://catalyst-team.github.io/catalyst/api/dl.html#catalyst.dl.core.runner.Runner) abstraction. <br/>\n",
    "It contains main logic about \"how\" you run the experiment and getting predictions.\n",
    "\n",
    "For supervised learning case, there is an extention for Runner – [SupervisedRunner](https://catalyst-team.github.io/catalyst/api/dl.html#module-catalyst.dl.runner.supervised), which provides additional methods like `train`, `infer` and `predict_loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Od4mIFlo0Edi"
   },
   "outputs": [],
   "source": [
    "from catalyst.dl.runner import SupervisedRunner\n",
    "\n",
    "runner = SupervisedRunner()\n",
    "\n",
    "# folder for all the experiment logs\n",
    "logdir = \"./logs/classification_tutorial_0\"\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FdVqh4ls0Edk"
   },
   "source": [
    "Using [Callbacks](https://catalyst-team.github.io/catalyst/api/dl.html#catalyst.dl.core.callback.Callback), the basic functionality of the catalyst can be expanded.\n",
    "\n",
    "A callback is a class inherited from `catalyst.dl.core.Callback` and implements one / several / all methods:\n",
    "\n",
    "```\n",
    "on_stage_start\n",
    "    --- on_epoch_start\n",
    "    ------ on_loader_start\n",
    "    --------- on_batch_start\n",
    "    --------- on_batch_end\n",
    "    ------ on_loader_end\n",
    "    --- on_epoch_end\n",
    "on_stage_end\n",
    "\n",
    "on_exception - if the code crashes with an error, you can catch it and reserve the parameters you need\n",
    "```\n",
    "\n",
    "You can find the list of standard callbacks [here](https://catalyst-team.github.io/catalyst/api/dl.html#module-catalyst.dl.callbacks.checkpoint). \n",
    "It includes callbacks such as\n",
    "- CheckpointCallback. Saves N best models in logdir\n",
    "- TensorboardLogger. Logs all metrics to tensorboard\n",
    "- EarlyStoppingCallback. Early training stop if metrics do not improve for the `patience` of epochs\n",
    "- ConfusionMatrixCallback. Plots ConfusionMatrix per epoch in tensorboard\n",
    "\n",
    "Many different metrics for classfication\n",
    "- AccuracyCallback\n",
    "- MapKCallback\n",
    "- AUCCallback\n",
    "- F1ScoreCallback\n",
    "\n",
    "segmentation\n",
    "- DiceCallback\n",
    "- IouCallback\n",
    "\n",
    "and many other callbacks, like LRFinder and MixupCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RZ3ISm8O0Edl"
   },
   "outputs": [],
   "source": [
    "# as we are working on classification task\n",
    "from catalyst.dl.callbacks import AccuracyCallback, AUCCallback, F1ScoreCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y7p0NLLt0Edm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runner.train(\n",
    "    model=model,\n",
    "    logdir=logdir,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    # We can specify the callbacks list for the experiment;\n",
    "    # For this task, we will check accuracy, AUC and F1 metrics\n",
    "    callbacks=[\n",
    "        AccuracyCallback(num_classes=num_classes),\n",
    "        AUCCallback(\n",
    "            num_classes=num_classes,\n",
    "            input_key=\"targets_one_hot\",\n",
    "            class_names=os.listdir(\"Images\")\n",
    "        ),\n",
    "        F1ScoreCallback(\n",
    "            input_key=\"targets_one_hot\",\n",
    "            activation=\"Softmax\"\n",
    "        )\n",
    "    ],\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xz2Q-s8E0Edn"
   },
   "source": [
    "### Training analysis and model predictions \n",
    "\n",
    "The `utils.plot_metrics` method reads tensorboard logs from the logdir and plots beautiful metrics with `plotly` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I5To-Wiz0Edo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from catalyst.dl import utils\n",
    "# it can take a while (colab issue)\n",
    "utils.plot_tensorboard_log(logdir, step='batch')\n",
    "# utils.plot_metrics(\n",
    "#     logdir=logdir, \n",
    "#     # specify which metrics we want to plot\n",
    "#     metrics=[\"loss\", \"accuracy01/_mean\", \"auc/_mean\", \"f1_score\", \"_base/lr\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jBbnjvLM0Edp"
   },
   "source": [
    "The method below will help us look at the predictions of the model for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wqkwCXHP0Edr"
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "def show_prediction(\n",
    "    model: torch.nn.Module, \n",
    "    class_names: List[str], \n",
    "    titles: List[str],\n",
    "    images: List[np.ndarray],\n",
    "    device: torch.device\n",
    ") -> None:\n",
    "    with torch.no_grad():\n",
    "        tensor_ = torch.stack([\n",
    "            valid_transforms(image=image)[\"image\"]\n",
    "            for image in images\n",
    "        ]).to(device)\n",
    "        \n",
    "        \n",
    "        logits = model.forward(tensor_)\n",
    "        probabilities = softmax(logits, dim=1)\n",
    "        predictions = probabilities.argmax(dim=1)\n",
    "    \n",
    "    images_predicted_classes = [\n",
    "        (f\"predicted: {class_names[x]} | correct: {title}\", image)\n",
    "        for x, title, image in zip(predictions, titles, images)\n",
    "    ]\n",
    "    show_examples(images_predicted_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFNtmD4M0Eds"
   },
   "outputs": [],
   "source": [
    "device = utils.get_device()\n",
    "titles, images = list(zip(*read_random_images(ALL_IMAGES)))\n",
    "titles = list(map(lambda x: x.rsplit(\":\")[0], titles))\n",
    "show_prediction(model, class_names=class_names, titles=titles, images=images, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BD1THkyZ0Edt"
   },
   "source": [
    "### Training with Focal Loss and OneCycle\n",
    "\n",
    "In the `catalyst.contrib` there are a large number of different additional criterions, models, layers etc\n",
    "\n",
    "For example,\n",
    "\n",
    "[catalyst.contrib.criterion](https://catalyst-team.github.io/catalyst/api/contrib.html#module-catalyst.contrib.criterion.ce):\n",
    "- HuberLoss\n",
    "- CenterLoss\n",
    "- FocalLossMultiClass\n",
    "- DiceLoss / BCEDiceLoss\n",
    "- IoULoss / BCEIoULoss\n",
    "- LovaszLossBinary / LovaszLossMultiClass / LovaszLossMultiLabel\n",
    "- WingLoss\n",
    "\n",
    "Lr scheduler in [catalyst.contrib.schedulers](https://catalyst-team.github.io/catalyst/api/contrib.html#module-catalyst.contrib.schedulers.base):\n",
    "- OneCycleLRWithWarmup\n",
    "\n",
    "Moreover, in [catalyst.contrib.models](https://catalyst-team.github.io/catalyst/api/contrib.html#models) you can find various models for segmentation:\n",
    "- Unet / ResnetUnet\n",
    "- Linknet / ResnetLinknet\n",
    "- FPNUnet / ResnetFPNUnet\n",
    "- PSPnet / ResnetPSPnet\n",
    "- MobileUnet\n",
    "\n",
    "\n",
    "Finally, several handwritten modules in [catalyst.contrib.modules](https://catalyst-team.github.io/catalyst/api/contrib.html#module-catalyst.contrib.modules.common):\n",
    "- Flatten\n",
    "- TemporalAttentionPooling\n",
    "- LamaPooling\n",
    "- NoisyLinear\n",
    "- GlobalAvgPool2d / GlobalMaxPool2d\n",
    "- GlobalConcatPool2d / GlobalAttnPool2d\n",
    "\n",
    "a bunch of others\n",
    "\n",
    "\n",
    "But for now, let's take `FocalLoss` and `OneCycleLRWithWarmup` to play around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Nr2tVa90Edt"
   },
   "outputs": [],
   "source": [
    "from catalyst.contrib.criterion import FocalLossMultiClass\n",
    "from catalyst.contrib.schedulers import OneCycleLRWithWarmup\n",
    "\n",
    "model = get_model(model_name, num_classes)\n",
    "\n",
    "criterion = FocalLossMultiClass()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "scheduler = OneCycleLRWithWarmup(\n",
    "    optimizer, \n",
    "    num_steps=NUM_EPOCHS,\n",
    "    lr_range=(0.001, 0.0001),\n",
    "    warmup_steps=1\n",
    ")\n",
    "\n",
    "# FocalLoss expects one_hot for the input\n",
    "# in our Reader function we have already created the conversion of targets in one_hot\n",
    "# so, all we need - respecify the target key name\n",
    "runner = SupervisedRunner(input_target_key=\"targets_one_hot\")\n",
    "logdir = \"./logs/classification_tutorial_1\"\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cq2SIp8f0Edu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runner.train(\n",
    "    model=model,\n",
    "    logdir=logdir,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    callbacks=[\n",
    "        AccuracyCallback(num_classes=num_classes),\n",
    "        AUCCallback(\n",
    "            num_classes=num_classes,\n",
    "            input_key=\"targets_one_hot\",\n",
    "            class_names=class_names\n",
    "        ),\n",
    "        F1ScoreCallback(\n",
    "            input_key=\"targets_one_hot\",\n",
    "            activation=\"Softmax\"\n",
    "        )\n",
    "    ],\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjSZj7QG0Edv"
   },
   "outputs": [],
   "source": [
    "# it can take a while (colab issue)\n",
    "utils.plot_tensorboard_log(logdir, step='batch')\n",
    "# utils.plot_metrics(\n",
    "#     logdir=logdir, \n",
    "#     # specify which metrics we want to plot\n",
    "#     metrics=[\"loss\", \"accuracy01/_mean\", \"auc/_mean\", \"f1_score\", \"_base/lr\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oadwRw-R0Edy"
   },
   "outputs": [],
   "source": [
    "device = utils.get_device()\n",
    "titles, images = list(zip(*read_random_images(ALL_IMAGES)))\n",
    "titles = list(map(lambda x: x.rsplit(\":\")[0], titles))\n",
    "show_prediction(model, class_names=class_names, titles=titles, images=images, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOeN7IRg0Edz"
   },
   "source": [
    "### Balancing classes in the dataset\n",
    "\n",
    "There are several useful data-sampler implementations in the `catalyst.data.sampler`. For example,\n",
    "- `BalanceClassSampler` allows you to create stratified sampling on an unbalanced dataset. <br/> A strategy can be either downsampling, upsampling or some prespeficied number of samples per class. <br/> Very important feature for every classification problem.\n",
    "- `MiniEpochSampler` allows you to split your \"very large dataset\" and sample some small portion of it every epoch. <br/> This is useful for those cases where you need to check valid metrics and save checkpoints more often. <br/> For example, your 1M images dataset can be sampled in 100k per epoch with all necessary metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g0Bn-V7T0Ed0"
   },
   "outputs": [],
   "source": [
    "from catalyst.data.sampler import BalanceClassSampler\n",
    "\n",
    "labels = [x[1] for x in train_dataset]\n",
    "sampler = BalanceClassSampler(labels, mode=\"upsampling\")\n",
    "\n",
    "# let's re-create our loaders with BalanceClassSampler\n",
    "loader = get_loaders(collate_fn=collate_fn, sampler=sampler)\n",
    "\n",
    "model = get_model(model_name, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "scheduler = OneCycleLRWithWarmup(\n",
    "    optimizer, \n",
    "    num_steps=NUM_EPOCHS, \n",
    "    lr_range=(0.001, 0.0001),\n",
    "    warmup_steps=1\n",
    ")\n",
    "\n",
    "runner = SupervisedRunner()\n",
    "logdir = \"./logs/classification_tutorial_2\"\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CuP-nR9B0Ed1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runner.train(\n",
    "    model=model,\n",
    "    logdir=logdir,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    callbacks=[\n",
    "        AccuracyCallback(num_classes=num_classes),\n",
    "        AUCCallback(\n",
    "            num_classes=num_classes,\n",
    "            input_key=\"targets_one_hot\",\n",
    "            class_names=class_names\n",
    "        ),\n",
    "        F1ScoreCallback(\n",
    "            input_key=\"targets_one_hot\",\n",
    "            activation=\"Softmax\"\n",
    "        )\n",
    "    ],\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H7gkH4JM0Ed2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# it can take a while (colab issue)\n",
    "utils.plot_tensorboard_log(logdir, step='batch')\n",
    "# utils.plot_metrics(\n",
    "#     logdir=logdir, \n",
    "#     # specify which metrics we want to plot\n",
    "#     metrics=[\"loss\", \"accuracy01/_mean\", \"auc/_mean\", \"f1_score\", \"_base/lr\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HbE_iUqp0Ed3"
   },
   "outputs": [],
   "source": [
    "device = utils.get_device()\n",
    "titles, images = list(zip(*read_random_images(ALL_IMAGES)))\n",
    "titles = list(map(lambda x: x.rsplit(\":\")[0], titles))\n",
    "show_prediction(model, class_names=class_names, titles=titles, images=images, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ESMoBjt40Ed4"
   },
   "source": [
    "## Model inference\n",
    "\n",
    "With SupervisedRunner, you can easily predict entire loader with only one method call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rBovQff30Ed5"
   },
   "outputs": [],
   "source": [
    "predictions = runner.predict_loader(\n",
    "    model, loaders[\"valid\"],\n",
    "    resume=f\"{logdir}/checkpoints/best.pth\", verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Ptv16Qk0Ed7"
   },
   "source": [
    "The resulting object has shape = (number of elements in the loader, output shape from the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRBEodCG0Ed7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z08z7zkV0Ed8"
   },
   "source": [
    "Thus, we can obtain probabilities for our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHDjuTSZ0Ed9"
   },
   "outputs": [],
   "source": [
    "print(\"logits: \", predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQX5YJxU0Ed-"
   },
   "outputs": [],
   "source": [
    "probabilities = torch.softmax(torch.from_numpy(predictions[0]), dim=0)\n",
    "print(\"probabilities: \", probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WYXh2E210Ed_"
   },
   "outputs": [],
   "source": [
    "label = probabilities.argmax().item()\n",
    "print(f\"predicted: {class_names[label]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GvxWb80srKKB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Classification-tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
