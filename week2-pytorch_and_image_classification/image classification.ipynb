{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan for today\n",
    "Today we will start working with [PyTorch](https://github.com/pytorch/pytorch). It’s a Python-based scientific computing package targeted at two sets of audiences:\n",
    "- A replacement for NumPy to use the power of GPUs\n",
    "- a deep learning research platform that provides maximum flexibility and speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch in 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy ndarrays -> PyTorch tensors\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand(5, 5)\n",
    "U, S, V = torch.svd(A)\n",
    "print(U)\n",
    "print(S) \n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of useful operations already in core pytorch! We highly recommend to read documentation about [tensors](https://pytorch.org/docs/stable/tensors.html) and [most common operations](https://pytorch.org/docs/stable/torch.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3, requires_grad=True)\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (x ** 2 + y).mean()\n",
    "print(loss)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = (x + y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = z.detach()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general rule - if any of input operands requires grad, the output tensor will also require grad. If all inputs do not require grad, the output will not require grad as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image classification pytorch starter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we need for deep learning?\n",
    "- dataset\n",
    "- neural network model\n",
    "- loss function (criterion)\n",
    "- optimization algorithm (gradient descent?)\n",
    "- training loop\n",
    "- save/load trained model\n",
    "- (optional) metrics\n",
    "- (optional) visualizations (will be discussed in future seminars)\n",
    "- (optional) learning rate schedule (will be discussed in future seminars)\n",
    "- (optional) results reproducibility (today)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As **Data** Scientists of some kind we should always start from the **data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab download link\n",
    "# !wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/35c067adcc1ab364c8803830cdb34d0d50eea37e/week01_backprop/mnist.py -O mnist.py\n",
    "import sys\n",
    "sys.path.insert(0, '../week1-backprop')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import load_dataset\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=False)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "plt.imshow(X_train[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch [dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) should implement two methods: `__len__()` and `__getitem__()`. Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        super().__init__()\n",
    "        assert len(X) == len(y)\n",
    "        self.x = X\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.x[idx], self.y[idx]\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Normalize to range [0, 1], reshape HWC to CHW\n",
    "    transforms.Normalize((0.5, ), (0.5, ))\n",
    "])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomResizedCrop(\n",
    "        size=(28,28),\n",
    "        scale=(0.9, 1.1),\n",
    "        ratio=(0.9, 1.1)\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, ))\n",
    "])\n",
    "\n",
    "train_dataset = MNISTDataset(X_train, y_train, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "x, y = train_dataset[0]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_tensor_image(t, normalize=True, range=(-1, 1), **kwargs):\n",
    "    if t.ndim == 4:\n",
    "        t = make_grid(t, normalize=normalize, range=range, **kwargs)\n",
    "    assert t.ndim == 3\n",
    "    img = np.transpose(t.numpy(), (1, 2, 0))\n",
    "    cmap = None\n",
    "    if img.shape[-1] == 1:\n",
    "        img = np.squeeze(img, axis=-1)\n",
    "        cmap = 'gray'\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    \n",
    "draw_tensor_image(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the training is done on batches, so we should use aggregate several examples into batch. This is done by [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_workers = 4  # set 0 for Windows (or enjoy bugs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,  # descendant of torch.utils.data.Dataset\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=None,  # how to create batch from separate examples\n",
    "    drop_last=False,  # drop incomplete batch\n",
    "    worker_init_fn=None,  # may be useful to set workers different random seeds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader))\n",
    "for batch_image, batch_target in train_loader:\n",
    "    break\n",
    "draw_tensor_image(batch_image)\n",
    "print(batch_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert also validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    MNISTDataset(X_val, y_val, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    MNISTDataset(X_test, y_test, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, 5, 2),  #12x12\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, 3),  #10x10\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, 3),  #8x8\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 128, 3, 2), #3x3\n",
    "    nn.ReLU(),\n",
    "    nn.modules.Flatten(),\n",
    "    nn.Linear(128*9, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "lr = 1e-3\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "loss_fn = nn.CrossEntropyLoss()  # combines nn.LogSoftmax() and nn.NCELoss()\n",
    "\n",
    "for e in range(epochs):\n",
    "    # timing\n",
    "    time_start = time.time()\n",
    "    # train\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for batch_image, batch_target in train_loader:\n",
    "        # zero_grad optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # forward\n",
    "        logits = model(batch_image)\n",
    "        loss = loss_fn(logits, batch_target)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # update all params\n",
    "        optimizer.step()\n",
    "        # update metrics\n",
    "        train_losses.append(loss.item())\n",
    "    train_loss = np.mean(train_losses)\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for batch_image, batch_target in valid_loader:\n",
    "        # forward\n",
    "        logits = model(batch_image)\n",
    "        loss = loss_fn(logits, batch_target)\n",
    "        val_losses.append(loss.item())\n",
    "    val_loss = np.mean(val_losses)\n",
    "    \n",
    "    # timing\n",
    "    epoch_time = time.time() - time_start\n",
    "    \n",
    "    print(f\"[Epoch {e:2d}]: loss={train_loss:.3f}, val_loss={val_loss:.3f}, epoch_time={epoch_time:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "# optimizer = optimizer.to(device)\n",
    "# loss_fn = loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loader(loader, train=True, device=device):\n",
    "    losses = []\n",
    "    for batch_image, batch_target in loader:\n",
    "        # move inputs to device\n",
    "        batch_image = batch_image.to(device)\n",
    "        batch_target = batch_target.to(device)\n",
    "        # forward\n",
    "        logits = model(batch_image)\n",
    "        loss = loss_fn(logits, batch_target)\n",
    "        # backward\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            # update all params\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # update metrics\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n",
    "epochs = 3\n",
    "lr = 1e-3\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)  # combines nn.LogSoftmax() and nn.NCELoss()\n",
    "model = model.to(device)\n",
    "\n",
    "for e in range(epochs):\n",
    "    # timing\n",
    "    time_start = time.time()\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = run_loader(train_loader, train=True)\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = run_loader(valid_loader, train=False)\n",
    "    \n",
    "    # timing\n",
    "    epoch_time = time.time() - time_start\n",
    "    \n",
    "    print(f\"[Epoch {e:2d}]: loss={train_loss:.3f}, val_loss={val_loss:.3f}, epoch_time={epoch_time:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it good or bad loss? We need **metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(logits, targets):\n",
    "    # TODO: implement\n",
    "\n",
    "def run_loader(loader, train=True, device=device):\n",
    "    model.train(train)\n",
    "    \n",
    "    losses = []\n",
    "    accs = []\n",
    "    for batch_image, batch_target in loader:\n",
    "        # move inputs to device\n",
    "        batch_image = batch_image.to(device)\n",
    "        batch_target = batch_target.to(device)\n",
    "        # forward\n",
    "        logits = model(batch_image)\n",
    "        loss = loss_fn(logits, batch_target)\n",
    "        # backward\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            # update all params\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # update metrics\n",
    "        losses.append(loss.item())\n",
    "        accs.append(acc(logits, batch_target).item())\n",
    "    \n",
    "    return np.mean(losses), np.mean(accs)\n",
    "\n",
    "epochs = 3\n",
    "lr = 1e-3\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)  # combines nn.LogSoftmax() and nn.NCELoss()\n",
    "model = model.to(device)\n",
    "\n",
    "for e in range(epochs):\n",
    "    # timing\n",
    "    time_start = time.time()\n",
    "    # train\n",
    "    train_loss, train_acc = run_loader(train_loader, train=True)\n",
    "    \n",
    "    # validation\n",
    "    val_loss, val_acc = run_loader(valid_loader, train=False)\n",
    "    \n",
    "    # timing\n",
    "    epoch_time = time.time() - time_start\n",
    "    \n",
    "    print(f\"[Epoch {e:2d}]: loss={train_loss:.3f}, val_loss={val_loss:.3f}, \"\n",
    "          f\"acc={train_acc:.3f}, val_acc={val_acc:.3f}, epoch_time={epoch_time:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use a better & faster model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(4*4*32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loader(model, optimizer, criterion, loader, train=True, device=device):\n",
    "    model.train(train)\n",
    "    \n",
    "    losses = []\n",
    "    accs = []\n",
    "    for batch_image, batch_target in loader:\n",
    "        # move inputs to device\n",
    "        batch_image = batch_image.to(device)\n",
    "        batch_target = batch_target.to(device)\n",
    "        # forward\n",
    "        logits = model(batch_image)\n",
    "        loss = criterion(logits, batch_target)\n",
    "        # backward\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            # update all params\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # update metrics\n",
    "        losses.append(loss.item())\n",
    "        accs.append(acc(logits, batch_target).item())\n",
    "    \n",
    "    return np.mean(losses), np.mean(accs)\n",
    "\n",
    "\n",
    "def train(model, optimizer, criterion, epochs=3, device=device):\n",
    "    criterion = criterion.to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        # timing\n",
    "        time_start = time.time()\n",
    "        # train\n",
    "        train_loss, train_acc = run_loader(model, optimizer, criterion, train_loader, train=True)\n",
    "\n",
    "        # validation\n",
    "        val_loss, val_acc = run_loader(model, optimizer, criterion, valid_loader, train=False)\n",
    "\n",
    "        # timing\n",
    "        epoch_time = time.time() - time_start\n",
    "\n",
    "        print(f\"[Epoch {e:2d}]: loss={train_loss:.3f}, val_loss={val_loss:.3f}, \"\n",
    "              f\"acc={train_acc:.3f}, val_acc={val_acc:.3f}, epoch_time={epoch_time:.2f}\")\n",
    "    \n",
    "    # test\n",
    "    test_loss, test_acc = run_loader(model, optimizer, criterion, test_loader, train=False)\n",
    "    print(f\"[Test]: loss={test_loss:.3f}, acc={test_acc:.3f}\")\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "\n",
    "lr = 1e-3\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train(model, optimizer, criterion, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './model.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)\n",
    "test_loss, test_acc = run_loader(model, optimizer, criterion, test_loader, train=False)\n",
    "print(f\"[Test]: loss={test_loss:.3f}, acc={test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "lr = 1e-3\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train(model, optimizer, criterion, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "lr = 1e-3\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train(model, optimizer, criterion, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "def set_random_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def prepare_cudnn(deterministic=True, benchmark=False):\n",
    "    if torch.cuda.is_available():\n",
    "        # CuDNN reproducibility\n",
    "        # https://pytorch.org/docs/stable/notes/randomness.html#cudnn\n",
    "        cudnn.deterministic = deterministic\n",
    "\n",
    "        # https://discuss.pytorch.org/t/how-should-i-disable-using-cudnn-in-my-code/38053/4\n",
    "        cudnn.benchmark = benchmark\n",
    "        \n",
    "def set_deterministic_behaviour(seed=42):\n",
    "    set_random_seed(seed)\n",
    "    prepare_cudnn(deterministic=True, benchmark=False)\n",
    "    \n",
    "# for n_workers>0 you also may need to set worker_init_fn=worker_init_fn in (train) DataLoader\n",
    "# def worker_init_fn(worker_id):\n",
    "#     set_random_seed(worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_deterministic_behaviour()\n",
    "epochs = 2\n",
    "\n",
    "lr = 1e-3\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train(model, optimizer, criterion, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_deterministic_behaviour()\n",
    "epochs = 2\n",
    "\n",
    "lr = 1e-3\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train(model, optimizer, criterion, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement grid neural architecture search on FasionMnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) is a bit harder dataset than MNIST, but with same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "train_dataset = FashionMNIST('./fmnist', train=True, download=True, transform=transform)\n",
    "test_dataset = FashionMNIST('./fmnist', train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(train_dataset))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_size = 10000\n",
    "train_indices, valid_indices = indices[:-val_size], indices[-val_size:]\n",
    "valid_dataset = # TODO\n",
    "train_dataset = # TODO\n",
    "print(len(train_dataset), len(valid_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = num_workers\n",
    "\n",
    "train_loader = # TODO\n",
    "valid_loader = # TODO\n",
    "test_loader = # TODO\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model parametrized by number of activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, activation_layer=nn.ReLU):\n",
    "        super(CNN, self).__init__()\n",
    "        # TODO\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, criterion, optimizer\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "epochs = 2\n",
    "test_loss, test_acc = train(model, optimizer, criterion, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform target into appropriate form https://pytorch.org/docs/stable/nn.html#torch.nn.MultiLabelMarginLoss\n",
    "def target_transform(target):\n",
    "    # TODO\n",
    "    return target\n",
    "\n",
    "class MultiLabelMarginLossCustom(nn.MultiLabelMarginLoss):\n",
    "    def forward(self, logits, target):\n",
    "        target = target_transform(target)\n",
    "        return super().forward(logits, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, criterion, optimizer\n",
    "model = CNN()\n",
    "criterion = MultiLabelMarginLossCustom()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "epochs = 2\n",
    "test_loss, test_acc = train(model, optimizer, criterion, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement grid search by activations and loss functions\n",
    "# can we trust the results? e.g. say that some activation/loss function is definetely better than other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
